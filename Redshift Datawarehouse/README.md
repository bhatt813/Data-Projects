
The README file includes a summary of the project, how to run the Python scripts, and an explanation of the files in the repository. Comments are used effectively and each function has a docstring.

# Project Summary
The analytics team is particularly interested in understanding what songs users are listening to. The first dataset is a subset of the Million Song Dataset. Each file is in JSON format and contains metadata about a song and the artist of that song. The second dataset consists of log files generated by an event simulator.

Project utilizes boto3 to create cluster, create a ARN and attach a policy, and work with S3 resources. The project creates and load staging tables using the COPY command to write data to the tables.  Creates final fact and dimension tables for more efficient reads for analytics. 

# Tables Created

## Fact Table
* [songplays] - songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
## Dimension Tables
* [users] - user_id, first_name, last_name, gender, level
* [songs] - song_id, title, artist_id, year, duration
* [artists] - artist_id, name, location, latitude, longitude
* [time] - start_time, hour, day, week, month, year, weekday
            
## Files
* test.ipynb displays the first few rows of each table to let you check your database.
* create_tables.py drops and creates your tables. You run this file to reset tables.
* ClusterCreate.ipynb walks through a Cluster Launch and runs create_tables and etl.py files
* etl.py reads and processes files from song_data and log_data and loads them into your tables.
* sql_queries.py contains all your sql queries, and is imported into the last three files above.
* dwh.cfg is the configuration file that contains the neccessary AWS credentials that are to be updated with private user credentials.
* README.md provides discussion on the project.

	
**Run Steps**: 
* If you do not have a redshift cluster created 
1. Run through Cluster Create file
2. Update Host and ARN information on dwh.cfg
2. Run create_tables.py
3. Run etl.py
4. verify data was inserted with test.ipynb
